\documentclass[journal, a4paper]{IEEEtran}
\usepackage{graphicx}   
\usepackage{url}
\usepackage{amsmath} 
\usepackage[utf8]{inputenc}
\usepackage{graphicx} 
\usepackage{float}
\usepackage{wrapfig}
\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{positioning}

\tikzset{%
   neuron missing/.style={
    draw=none, 
    scale=4,
    text height=0.333cm,
    execute at begin node=\color{black}$\vdots$
  },
}

\pgfplotsset{width = 5cm, compat=1.13}
\pgfplotsset{every axis/.append style={
    axis x line=middle,    % put the x axis in the middle
    axis y line=middle,    % put the y axis in the middle
    axis line style={<->}, % arrows on the axis
    },
    cmhplot/.style={color=blue,mark=none,line width=1pt,<->},
}

\begin{document}
	\title{Multimedia processing using machine learning}
	\author{Jakub Bielawa, Paweł Cejrowski, Łukasz Dawidowski, Łukasz Myśliński, Agata Radys}
	\markboth{Gdansk University of Technology, ETI department}{}
	\maketitle

\begin{abstract}
Some abstract
\end{abstract}

\section{Introduction}
\section{Theory}
\subsection{Features, dependencies, correlation etc.}
\subsection{Neural networks}
\subsubsection{Fully connected neural network}
todo
\subsubsection{Convolutional neural network}
Convolutional neural network (CNN) is a type of feed-forward artificial neural network, which means that connections between the neurons do not form a cycle. Information in the network moves only forward so the given signal goes through the neuron once.
\par CNN includes a convolutional layer and usually few other hidden layers. Each neuron is connected only to a small region of the previous layer called receptive field. Receptive fields of different neurons are overlapping with other neuron's fields so that together they are covering the whole input.
\par CNN are used for image processing because while using convolution they can recognize edges of an object on the image.
\begin{figure}[H]
\centering
\includegraphics[width=250px]{pictures/cnn.png}
\caption{Convolutional neural network}
\end{figure}

\subsubsection{Recurrent neural network}
In recurrent neural network (RNN) connections between the neurons form a directed cycle. It means that RNN can use its internal memory to learn  sequences of inputs. The same set of weights is applied recursively to the structure.
\par RNN are used for text or speech recognition.
\begin{figure}[H]
\centering
\includegraphics[width=250px]{pictures/rnn.png}
\caption{Convolutional neural network}
\end{figure}

\subsection{Activation function}
The activation function defines the output of considered neuron with given input. There are many functions that can be used as an activation function in artificial neural networks. The most popular are:
\subsubsection{Linear}
\begin{center}
$ f(x)=ax + b $ \\ $ a,b \in R $ \\~\\
\begin{tikzpicture}
	\begin{axis}
		\addplot[cmhplot,-]{x};
	\end{axis}
\end{tikzpicture}
\end{center}

\subsubsection{Rectified Linear Unit (ReLU)}
\[
f(x)=
\left\{
\begin{array}{ll}
      0 ,& x < 0 \\
      x ,& x\geq 0 \\
\end{array} 
\right. \]
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
            xmin=-4,xmax=4,
            ymin=-4,ymax=4,
        ]
        \addplot[cmhplot,-,domain=-4:0]{0};
        \addplot[cmhplot,-,domain=0:4]{x};

    \end{axis}
\end{tikzpicture}
\end{center}

\subsubsection{Sigmoid}
\begin{center}
$f(x) = \frac{1}{1 + e^{-x}}$ \\
\begin{tikzpicture}
    \begin{axis}[
            xmin=-5,xmax=5,
            ymin=-1,ymax=1,
        ]
        \addplot[cmhplot,-]{1/(1 + e^(-x))};
    \end{axis}
\end{tikzpicture}
\end{center}

\subsubsection{TanH}
\begin{center}
$ f(x)=\tanh(x)=\frac{2}{1+e^{-2x}}-1 $ \\
\begin{tikzpicture}
    \begin{axis}[
            xmin=-5,xmax=5,
            ymin=-1,ymax=1,
        ]
        \addplot[cmhplot,-]{2/(1 + e^(-2*x))-1};
    \end{axis}
\end{tikzpicture}
\end{center}

\section{Architecture}
The solution for our problem is fully connected 4-hidden-layer neural network.
\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]

\foreach \m/\l [count=\y] in {1,2,3}
{
 \node [circle,fill=green!50,minimum size=0.2cm] (input-\m) at (0,1.25-0.5*\y) {};
}
\foreach \m/\l [count=\y] in {4}
{
 \node [circle,fill=green!50,minimum size=0.2cm ] (input-\m) at (0,-1.25-\y) {};
}
 
 \node [neuron missing]  at (0,-1.25) {};
 

\foreach \m [count=\y] in {1,2,3}
  \node [circle,fill=red!50,minimum size=0.2cm ] (hidden1-\m) at (1,1.75-0.5*\y) {};
  
\foreach \m [count=\y] in {4}
  \node [circle,fill=red!50,minimum size=0.2cm ] (hidden1-\m) at (1,-3.25) {};
  
 \node [neuron missing]  at (1,-1.25) {};



\foreach \m [count=\y] in {1,2,3}
  \node [circle,fill=red!50,minimum size=0.2cm ] (hidden2-\m) at (2,1.75-0.5*\y) {};
  
\foreach \m [count=\y] in {4}
  \node [circle,fill=red!50,minimum size=0.2cm ] (hidden2-\m) at (2,-3.25) {};
  
 \node [neuron missing]  at (2,-1.25) {};



\foreach \m [count=\y] in {1,2,3}
  \node [circle,fill=red!50,minimum size=0.2cm ] (hidden3-\m) at (3,1.5-0.5*\y) {};
  
\foreach \m [count=\y] in {4}
  \node [circle,fill=red!50,minimum size0.2cm ] (hidden3-\m) at (3,-3.0) {};
  
 \node [neuron missing]  at (3,-1.25) {};



\foreach \m [count=\y] in {1,2,3}
  \node [circle,fill=red!50,minimum size=0.2cm ] (hidden4-\m) at (4,1.25-0.5*\y) {};
  
\foreach \m [count=\y] in {4}
  \node [circle,fill=red!50,minimum size=0.2cm ] (hidden4-\m) at (4,-2.75) {};
  
 \node [neuron missing]  at (4,-1.25) {};



% output layer
\foreach \m [count=\y] in {1}
  \node [circle,fill=blue!50,minimum size=0.2cm ] (output-\m) at (5,1.5-\y) {};
\foreach \m [count=\y] in {2}
  \node [circle,fill=blue!50,minimum size=0.2cm ] (output-\m) at (5,1.0-\y) {};
\foreach \m [count=\y] in {3}
  \node [circle,fill=blue!50,minimum size=0.2cm ] (output-\m) at (5,0.5-\y) {};
\foreach \m [count=\y] in {4}
  \node [circle,fill=blue!50,minimum size=0.2cm ] (output-\m) at (5,0.0-\y) {};
\foreach \m [count=\y] in {5}
  \node [circle,fill=blue!50,minimum size=0.2cm ] (output-\m) at (5,-0.5-\y) {};
\foreach \m [count=\y] in {6}
  \node [circle,fill=blue!50,minimum size=0.2cm ] (output-\m) at (5,-1.0-\y) {};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% labels

\foreach \l [count=\i] in {1,2,3,169}
  \draw [<-] (input-\i) -- ++(-1,0)
    node [above, midway] {$I_{\l}$};

\foreach \l [count=\i] in {1}
  \node [above] at (hidden1-\i.north) {$H_{\l}$};

\foreach \l [count=\i] in {1}
  \node [above] at (hidden2-\i.north) {$H_{\l}$};

\foreach \l [count=\i] in {1}
  \node [above] at (hidden3-\i.north) {$H_{\l}$};

\foreach \l [count=\i] in {1}
  \node [above] at (hidden4-\i.north) {$H_{\l}$};

\foreach \l [count=\i] in {1024}
  \node [below] at (hidden1-4.south) {$H_{\l}$};

\foreach \l [count=\i] in {1024}
  \node [below] at (hidden2-4.south) {$H_{\l}$};

\foreach \l [count=\i] in {256}
  \node [below] at (hidden3-4.south) {$H_{\l}$};

\foreach \l [count=\i] in {56}
  \node [below] at (hidden4-4.south) {$H_{\l}$};



\foreach \l [count=\i] in {1,2,3,4,5,6}
  \draw [->] (output-\i) -- ++(1,0)
    node [above, midway] {$O_{ \l}$};
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% connections

\foreach \i in {1,...,4}
  \foreach \j in {1,...,4}
    \draw [->] (input-\i) -- (hidden1-\j);

\foreach \i in {1,...,4}
  \foreach \j in {1,...,4}
    \draw [->] (hidden1-\i) -- (hidden2-\j);
\foreach \i in {1,...,4}
  \foreach \j in {1,...,4}
    \draw [->] (hidden2-\i) -- (hidden3-\j);
		\foreach \i in {1,...,4}
  \foreach \j in {1,...,4}
    \draw [->] (hidden3-\i) -- (hidden4-\j);
\foreach \i in {1,...,4}
  \foreach \j in {1,...,6}
    \draw [->] (hidden4-\i) -- (output-\j);

\end{tikzpicture}

	
\section{Experiments}

\section{Conclusions}
\section{Bibliography}
\end{document}
